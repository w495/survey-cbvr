Что это
========

Исходники статьи в TeX. Этот файл, кстати, тоже получен из TeX.


Abstract
==========


The paper focuses on an overview of the different existing methods in content-based video retrieval. During the last decade there was a rapid growth of video posted on the Internet. This imposes urgent demands on video retrieval. Video has a complex structure and can express the same idea in different ways. This makes the task of searching for video more complicated. Video titles and text descriptions cannot give the hole information about objects and events in the video. This creates a need for content-based video retrieval. There is a semantic gap between low-level video features, that can be extracted, and the users’ perception. Complex content-based video retrieval can be regarded as the bridge between traditional retrieval and semantic-based video retrieval.

<span>**Keywords:**</span> frames; near-duplicates video; scenes; shots; video annotation; video classification; video mining; video reranking; video retrieval;


Аннотация
==========

УДК 004.932.4

В статье предлагается обзор различных существующих методов ассоциативного поиска по видео. В течение прошлого десятилетия наблюдался стремительный рост количества видео размещаемых в Интернете, что создало острую необходимость в появлении поиска по видео. Видео имеет сложную структуру. Одна и та же информация может быть выражен различными способами. Это серьёзно усложняет задачу видео-поиска. Заголовки и описания видео не могут дать полного представления о самом видео, что влечет за собой необходимость использования ассоциативного поиска по видео. Существует семантический разрыв между низкоуровневыми характеристиками видео и восприятием пользователей. Комплексный ассоциативный видео-поиск может рассматриваться как связующее звено между обычным поиском и смысловым поиском по видео.

<span>**Ключевые слова:**</span> анализ видео; аннотирование видео; видео-поиск; кадры; классификация видео; нечеткие дубликаты видео; ранжирование видео; сцены; съёмки;

Введение
========

В связи с увеличением пропускной способности сетей, многие пользователи получили доступ к видео в Интернете. Для примера, каждую минуту на сайт YouTube  загружаются более 48 часов новых видео. Более 14 миллиардов клипов были просмотрены в мае 2010.

В длинном видео сложно автоматизировано найти интересующий отрывок. А, размечать и искать видео вручную весьма трудоемко. Смысловой разрыв между низкоуровневой информацией и потребностями пользователя, заставляет работать с видео на более высоком уровне. Тем не менее, большинство методов поиска следуют парадигме прямого отображения низкоуровневых характеристик видео на смысловые понятия. Этот подход требует предварительной обработки данных. А результаты такого отображения не будут устойчивы. Без учета конкретной предметной области задача кажется неразрешимой. Последнее время стало появляться много клипов с очень схожим содержанием (нечеткие дубликаты видео).

Задача эффективной идентификации нечетких дубликатов играет ключевую роль в задачах поиска, защите авторских прав, и многих других. Анализа большого объема видео-данных для выделения нужной информации является сложной задачей. Для ее решения применяют ассоциативный поиск. В англоязычной литературе ассоциативный видео-поиск называют «content based video retrieval» (CBVR) — поиск по содержимому.

Ассоциативный поиск используется для автоматического реферирования видео, анализа новостных событий, видеонаблюдения, и в образовательных целях Dimitrova et al. (2002).

Видео содержит в себе несколько видов данных. Авторы Chung et al. (2007) и A. F. Smeaton (2006) выделяют четыре вида.

1.  Метаданные — заголовок, автор и описание;

2.  Звуковая дорожка;

3.  Тексты полученные при помощи технологии оптического распознавания символов (OCR);

4.  Визуальная информация кадров видео.

Таким образом, видео обладает комплексностью. Комплексность (системность, мультимодальность) — способность взаимодействовать с пользователем по различным каналам информации и извлекать и передавать смысл автоматически Nigay and Coutaz (1993).

Комплексность видео состоит в возможности автора выражать мысли, используя по крайней мере два информационных канала. Каналы могут быть визуальными, звуковыми или текстовыми.

В работе T. Zhang et al. (2012) дают хороший обзор аннотации видео. В работе Tian et al. (2011) описываются свежие исследования методов ранжирования видео.

Ассоциативный поиск видео состоит из следующих шагов.

1.  Анализ временной структуры видео — деление видео на фрагменты, которое включает обнаружение границ съёмок.

2.  Определение характеристик фрагментов.

3.  Извлечение информации из характеристик.

4.  Аннотация видео, построение семантического индекса.

5.  Обработка пользовательского запроса и выдача результата.

6.  Обратная связь и переранжирование результатов для улучшения поиска характеристик.

Деление видео
=============

Деление видео включает в себя обнаружение границ съёмок, извлечение ключевых кадров, сегментацию сцен и аудио.

Обнаружение границ съёмок
-------------------------

Видео делят на фрагменты по времени. В качестве таких фрагментов могут выступать съёмки. Съёмка (кинематографический кадр, монтажный план) — отрезок киноплёнки, на котором запечатлено непрерывное действие между пуском и остановкой камеры, или между двумя монтажными склейками.

С точки зрения семантики, самым мелким элементом видео является кадр (фотографический кадр, кадрик). Съёмка является более крупным делением. Из съёмок складываются сцены, а из сцен видео целиком.

Границы съёмок бывают трех типов:

-   линейная склейка — съёмка внезапно прерывается и начинается другая;

-   постепенное исчезновение или проявление (в монохромном кадре);

-   вытеснение — исчезновения одной съёмки, и появления другой (растворение, вытеснение шторкой).

Для обнаружения границ съёмок, как правило, сначала извлекают визуальные характеристики каждого кадра. Затем, на основе выделенных признаков, оценивают сходство между кадрами. Границы съёмок определяют по смене неоднородных кадров. В работе Nigay and Coutaz (1993) описаны параметры смены кадров и ошибки выделения на основе глобальных и локальных характеристик для обнаружения съёмок и классификации.

Существует два типа методов обнаружения съёмок.

1.  Пороговые — попарно сравнивают подобия кадров с заданным порогом.

2.  Статистические — обнаруживают границы сцен на основе характеристик кадров.

Извлечение ключевых кадров
--------------------------

Среди кадров одной съёмки есть избыточность. Для ее уменьшения выделяют кадры, которые наиболее полно отражают содержание съёмки.

При извлечении ключевых кадровиспользуют различные характеристики:

-   цветовые гистограммы;

-   края;

-   очертания;

-   оптические потоки.

Способы извлечения подразделяются на шесть категорий:

-   последовательное сравнение;

-   глобальное сравнение;

-   на основе ссылочных кадров;

-   на основе кластеризации;

-   на основе упрощения кривых;

-   и на основе объектов или событий Truong and Venkatesh (2007).

При последовательном сравнении ключевой кадр сравнивают с другими кадрами до тех пор пока не будет найден «сильно отличный». Для сопоставления кадров используется цветовые гистограммы X.-D. Zhang et al. (2003).

Методы глобального сравнения используют различия между кадрами в съёмке и распределяют ключевые кадры, минимизируя предопределенную целевую функцию.

Методы на основе ссылочных кадров генерируют систему отсчета кадров и затем сравнивают ключевые кадры съёмки со ссылочным.

В работе Matsumoto et al. (2006) описано создание средней гистограммы без канала прозрачности. С помощью такой гистограммы описывается цветовое распределение кадров в съёмке.

Сегментация сцен
----------------

Сегментация сцен также известна как деление сюжета на блоки.

Сцена представляет собой группу смежных съёмок. Эти съёмки связаны между собой конкретной темой или предметом. Сцены обладают семантикой более высокого уровня чем съёмки.

Существует три способа сегментации сцен:

-   деление по ключевому кадру;

-   деление на основе объединения визуальной и звуковой информации;

-   деление по фону.

При делении сцен по ключевому кадру каждая съёмка представляется набором ключевых кадров. Для кадров выявляют их характеристики. Близкие по времени кадры с близкими характеристиками группируют в сцены Truong, Venkatesh, and Dorai (2003). Далее, используя сравнение блоков ключевых кадров, вычисляют сходство между съёмками, Ограничение деления по ключевому кадру заключается в том, что кадры не могут эффективно представить динамическое содержание съёмки.

Съёмки в пределах сцены, как правило, связаны динамическим развитием сюжета в пределах сцены, а не сходством ключевых кадров.

При одновременном анализе звуковой и визуальной информации сменой сцен считают границу съёмки, где содержимое обоих каналов изменяется одновременно. Для определения соответствия между этими двумя наборами сцен используют алгоритм поиска ближайшего соседа с ограничением по времени Sundaram and Chang (2000). К минусам подхода следует отнести сложность определения связи между аудио сегментами и визуальными съёмками.

Деление сцен по фону основано на гипотезе, что съёмки, принадлежащие к одной сцене часто имеют один и тот же фон. Для восстановления фона каждого кадра используют объединение близких по цвету пикселей в одноцветные прямоугольные области. Сходство съёмок определяют с помощью оценки распределения цвета и текстуры всех фоновых изображений в кадре. Для управлением процессом группировки съёмок применяют кинематографические правила L.-H. Chen, Lai, and Mark Liao (2008).

Сегментация звука
-----------------

Звуковая дорожка — богатый источник информации о содержании для всех жанров видео.

Как показано в лингвистической литературе границы «высказываний» выделяются интонационно. На существенные изменения темы обычно указывают:

-   длинные паузы;

-   изменения тона;

-   и более изменением амплитуды колебаний.

Для автоматического деления речи на темы применяется вероятностная модель связи интонационных и лексических сигналов. Сначала извлекают большое количество интонационных характеристик. И, таким образом, получают два главных типа речевой просодии: продолжительность и тон.

На основе дерева принятия решений выбирают типичную интонационную функцию. После чего, лексическая информация извлекается с помощью Скрытых Моделей Маркова(HMM) и статистических моделей языка.

Аудио является перспективным источником информации для анализа лекционных видео. Обычно такие видео длятся \(60 – 90\) минут. Сложно искать интересующий отрывок по всему такому видео Repp, Grob, and Meinel (2008). Для решения проблемы используют технологии распознавания речи. Сначала текст извлекают из аудио, а потом производят индексацию стенограммы для поиска по ней Kamabathula and Iyer (2011). Например, система распознавания речи Sphinx-4 при поиске по видео достигает полноты 72% и средней точности 84%.

Выделение признаков
===================

Из полученных частей видео выделяют признаки. К признакам относят:

-   характеристики ключевых кадров;

-   объекты;

-   движение в кадре;

-   характеристики аудио и текста.

Характеристики ключевых кадров
------------------------------

Выделяют цветовые, текстурные, формовые, краевые характеристики.

### Цветa

Цветовые характеристики включают в себя цветовые гистограммы, цветовые моменты, цветовые коррелограммы, смесь Гауссовых моделей. При выделении локальной цветовой информации изображения разбивают на блоки \(5 \times 5\) R. Yan and Hauptmann (2007).

### Текстуры

Текстурными характеристиками называют визуальные особенности поверхности некоторого объекта. Они не зависят от тона или насыщенности цвета объекта. Текстурные характеристики отражают однородные явления в изображениях. Для выделение текстурной информации из видео применяют фильтры Габора Adcock et al. (2004).

### Контуры

Контурные или формовые характеристики, описывают формы объектов в изображениях. Они могут быть извлечены из контуров или областей объектов.

### Края

На конференции TRECVid-2005 для получения пространственного распределение краев в задаче поиска по видео был предложен дескриптор гистограммы границ (EHD) A. G. Hauptmann et al. (2003).

Характеристики объектов
-----------------------

Такие характеристики включают параметры областей изображения, которые соответствуют объектам:

-   основной цвет;

-   текстуру;

-   размер и т.д.

В работе J. Sivic, Everingham, and Zisserman (2005) предложена система поиска лиц. По видео-запросу с конкретным человеком система способна выдать ранжированный список съёмок с этим человеком. Текстовая индексация и поиск приводят к расширению семантики запроса и делают возможным использования Glimpse-метода (agrep), для поиска нечеткого соответствия H. Li and Doermann (2002).

Характеристики движения
-----------------------

Характеристики движения ближе к смысловым понятиям, чем характеристики ключевых статических кадров и объектов. Движение в видео может быть вызвано движением камеры и движением предметов в кадре.

Движения камеры такие как «приближение или удаление», «панорамирование влево или вправо» и «смещение вверх или вниз» используются для индексации видео. Движения объектов на данный момент являются предметном исследований.

Звуковые характеристики
-----------------------

Преимущество аудио-подходов состоит в том, что они обычно требуют меньше вычислительных ресурсов, чем визуальные методы. Кроме того, аудио-записи могут быть очень короткими.

Многие звуковые характеристики выбраны на основе человеческого восприятие звука. Характеристики аудио можно разделить на три уровня L.-H. Chen, Lai, and Mark Liao (2008):

-   низкоуровневая акустика, такая как средняя частота для кадра,

-   средний уровень, как признак объекта, например звук скачущего мяча,

-   высокоуровневые, такие как речь и фоновая музыка, играющая в определенных типах видео.

В работе Seyerlehner et al. (2010) используют блочные характеристики аудио. Аудио-поток при этом разделяется на отрезки в 2048 отсчетов. Для выделения таких характеристик применяют функцию Ханнаи логарифмическую шкалу.

Представление видео
===================

В работе Haase, Davis, and Davis (1995) была сформулировала проблема машинного представления видео. В работе Su et al. (2007) разработаны многослойные, графические аннотации видео — мультимедийные потоки. Они представляют собой визуальный язык как способ представления видеоданных. Особое внимание уделено проблеме создания глобального архива видео, допускающего повторное использование. Нисходящие поисковые системы используют высокоуровневые знания определенной предметной области, чтобы генерировать надлежащие представления.

Но как было сказано выше, это не самый удобный подход. Представление, управляемое данными — стандартный способ извлечь низкоуровневые характеристики и получить соответствующие представления без любых предварительных знаний о предметной области.

Представления, на основе данных могут быть сведены к двум основным классам.

1.  Сигнальные признаки, которые характеризуют низкоуровневое аудиовизуальное содержание. К ним можно отнести цветовые гистограммы, формы, текстуры,

2.  Описательное представление с помощью текста, атрибутов или ключевых слов. Авторы работы Cheng and Chia (2011) предлагают для описания видео использовать послойные графовые клики ключевых кадров (SKCs), которые более компактны и информативны, чем последовательность изображений или ключевые кадры.

Анализ видео
============

Интеллектуальный анализ данных в больших базах видео стал доступен недавно. Задачи анализа видеоинформации можно сформулировать как выявление:

-   структурных закономерностей видео;

-   закономерностей поведения движущихся объектов;

-   характеристик сцены;

-   шаблонов событий и их связей;

-   и других смысловых атрибутов в видео.

В работах применяют извлечение объектов — группировку различных экземпляров того же объекта, который появляется в различных частях видео.

Для классификации пространственных характеристик кадров применяют метод поиска ближайших соседейAnjulan and Canagarajah (2009).

Обнаружение специальных шаблонов применяется к действиям и событиям, для которых есть априорные модели, такие как действия человека, спортивные мероприятия, дорожные ситуации или образцы преступлений Quack, Ferrari, and Gool (2006).

Поиск моделей — автоматическое извлечение неизвестных закономерностей в видео. Для поиска моделей используют экспертные системы с безнадзорнымили полуконтролируемым обучением.

Поиск неизвестных моделей полезен для изучения новых данных в наборе видео. Неизвестные образцы обычно находят благодаря кластеризации различных векторов характеристик.

Для выявления закономерностей поведения движущихся объектов используют n-граммы и суффиксные деревья. При этом анализируют последовательности событий по многократным временным масштабам.

Классификация видео
===================

Задача классификации состоит в том, чтобы отнести видео к предопределенной категории. Для этого используют характеристики видео или результаты интеллектуального анализа данных.

Классификация видео — хороший способ увеличить эффективность видео-поиска. Семантический разрыв между низкоуровневыми данными и интерпретацией наблюдателя, делает ассоциативную классификацию очень трудной задачей.

Смысловая классификация видео может быть выполнена на трех уровнях Tamizharasan and S (2013):

-   жанры

    -   например, «фильмы», «новости», «спортивные соревнования», «мультфильмы», «реклама» и т.д.

-   события видео;

-   и объекты в видео.

Жанры
-----

Жанровая классификация разделяет видео на подмножество соответствующее жанру и несоответствующее J. Wu and Worring (2012).

В работе Jiang, Ngo, and Yang (2007) предложена классификация большого числа видео только по заголовку видео. Для этого использован поэтапный метод опорных векторов.

Видео классифицируют также на основе статистических моделей различных жанров. Для этого анализируют структурные свойства: статистику цвета, съёмки, движение камеры и объектов. Свойства используются, чтобы получить более абстрактные атрибуты стиля. К абстрактным атрибутам стиля можно отнести: панорамирование камеры и изменение масштаба, речь и музыку. Строят отображение этих атрибутов на жанры видео.

В работе Ionescu et al. (2012) для классификации жанров используется комбинация из четырех дескрипторов:

-   блоковый аудио дескриптор

    -   захватывает локальную временную информацию;

-   дескриптор визуальной временной структуры

    -   использует информацию о смене съёмок,

    -   оценивает количество съёмок за определенный интервал времени («ритм» видео),

    -   описывает об «активные» и «не актиные» смены съемок;

-   дескриптор цвета

    -   использует статистику распределения цвета, элементарных оттенков, цветовых свойств, и отношений между цветами;

-   статистика фигур контуров.

Были проведены эксперименты на видеоматериалах общей продолжительностью 91 часе видео. Классификация проводилась на семи жанрах видео: мультфильмы, реклама, документальные фильмы, художественные фильмы, музыкальные клипы, спортивные соревнования и новости. Комплексный дескриптор позволил авторам достичь точности 87% -100% и полноты 77% -100%.

События
-------

Событие может быть определено как любое явление в видео, которое

-   может быть воспринято зрителем;

-   играет роль для представления содержимого.

Каждое видео может состоять из многих событий, и каждое событие может состоять из многих подсобытий. Таким образом складывается иерархическая модель P. Chang, Han, and Gong (2002).

Объекты
-------

Объектная классификация является самым низкоуровневым типом классификации. Съёмки классифицируют тоже на основе объектов. Объекты в съёмках представлены с помощью параметров цвета, текстуры и траектории. В работе G. Hong, Fong, and Fong (2005) для кластеризации связанных съёмок используется нейронная сеть. Каждый кластер отображен на одну из 12 категорий. Объекты разделяются по положению в кадре и характеру движения.

Аннотирование видео
===================

Процесс присваивания переопределенных смысловых понятий фрагментам видео называют аннотированием. Примеры смысловых понятий : человек, автомобиль, небо и гуляющие люди.

Аннотирование видео подобно классификации, за исключением двух различий.

1.  Для классификаций важны жанры, а для аннотирования понятия. Жанры и понятия имеют различную природу, несмотря на то, что некоторые методы могут быть использованы в обоих задачах.

2.  Классификация видео применяется к полным видео, в то время как аннотируют обычно фрагменты Yang et al. (2007).

Аннотирование, основанное на обучении, необходимо для анализа и понимания видео. Было предложено много различных способов автоматизации процесса.

Например, в работе T. Zhang et al. (2012) было разработано «быстрое полуконтролируемоеграфовое обучение на нескольких экземплярах» (Fast Graphbased Semi-Supervised Multiple Instance Learning — FGSSMIL). Алгоритм работает в рамках общей платформы для разных типов видео одновременно (спортивные передачи, новости, художественные фильмы). Для обучения модели используется небольшое число видео, размеченных вручную, и значительный объем не размеченного материала.

В работе Weal et al. (2012) предлагается создавать частичную ручную аннотацию видео как часть практической профессиональной подготовки. Авторы рассматривают лабораторные занятия студентов-медиков. Во время занятия идет запись видео. Кроме того одновременно происходит запись изменения состояния тренировочного манекена (виртуального пациента). Таким образом, к записанному видео добавляется семантическая разметка на основе показаний датчиков манекена. После происходит разбор занятия и анализ допущенных ошибок, В результате к видео добавляется разметка, созданная самими студентами.

Обработка запроса
=================

После построения поискового индекса может быть выполнен ассоциативный поиск. Поисковая выдача оптимизируются на основе связи между запросами.

Типы запросов
-------------

Существует две категории запросов: семантические и не семантические.

### Семантические запросы

К семантическим запросам относят наборы ключевых слов и поисковые фразы. Ключевые слова — наиболее очевидный и простой вид запроса. При таких запросах частично учитывается семантика видео. Поисковые фразы или запросы на естественном языке — самый естественный и удобный способ взаимодействия человека с поисковой системой. Для выбора и ранжирования видео используется смысловая близость слов Aytar, Shah, and Luo (2008).

### Не семантические запросы

Не семантические запросы используются для поиска по образцу, эскизам, объектам и т.д.. Запросом может быть изображение или видео.

#### Поиск по образцу

При таком поиске из запроса выделяют низкоуровневые характеристики и сравнивают их с данными в базе с помощью меры сходства.

#### Поиск по эскизу

Пользователи могут изобразить нужное видео с помощью эскиза. Далее для эскиза применяется поиск по образцу.

#### Поиск объекта

В качестве запроса выступает изображение объекта. Система находит и возвращает все вхождения объекта в материалах из базы Josef Sivic and Zisserman (2006). В отличие от предыдущих видов запросов, в данном случае, привязка происходит не к видео, а именно по изображенному объекту.

Оценка сходства
---------------

Критерии близости видео является важным фактором при поиске. Выделяют несколько способов сравнения видео:

-   сравнение характеристик;

-   сравнение текста;

-   сравнение онтологий.

Применяют также комбинации методов. Выбор конкретного метода зависит от типа запроса.

Сравнение характеристик
-----------------------

При сравнение характеристик видео оценивают среднее расстояние между особенностями соответствующих кадров Browne and Smeaton (2005).

Сравнение текста
----------------

Для сравнения запроса и описания видео применяют текстовое сопоставление. Описание и запрос нормализуют, а затем вычисляют их смысловое сходство, используя пространственные векторные модели C. G. M. Snoek et al. (2007).

Сравнение онтологий
-------------------

При сравнении онтологии оценивают смысловое сходство отношений между ключевыми словами запроса и описанием аннотированного видео Aytar, Shah, and Luo (2008).

Для усиления влияния смысловых понятий автоматически подбирают комбинации методов. Для этого исследуют различные стратегии на учебном наборе видео.

Оценка релевантности
--------------------

Видео из поисковой выдачи оцениваются или пользователем или автоматически. Эту оценку используют для уточнения дальнейших поисков. Обратная связь релевантности устраняет разрыв между смысловым понятием адекватности поискового ответа и низкоуровневым представлением видео.

Явная обратная связь предлагает пользователю выбрать релевантные видеоролики из ранее полученных ответов. На основе мнений пользователей в системы меняют коэффициенты мер подобия L.-H. Chen, Chin, and Liao (2008).

Неявная обратная связь уточняет результаты поиска на основе кликов и переходов пользователя.

Псевдообратная связьвыделяет положительные и отрицательные выборки из предыдущих результатов поиска без участия человека.

Рассматривая текстовую и визуальную информацию с вероятностной точки зрения, визуальное ранжирование можно сформулировать как задачу байесовской оптимизации. Такое прием называют байесовским визуальным ранжированием.

Заключение
==========

Многие вопросы остаются открытыми и требуют дальнейшего исследования, особенно в следующих областях.

Большинство современных подходов индексации видео сильно зависят от предварительных знаний о предметной области. Это ограничивает их расширяемость для новых областей. Устранение зависимости от предварительных знаний — важная задача будущих исследований.

Индексация и поиск видео в среде «облачных» вычислений сформировали новое направление исследований видео-поиска. Важной особенностью «облачных» вычислений является то, что искомые видео и сама база данных меняются динамически.

Современные подходы к смысловому поиску видео, как правило, используют набор текстов для описания визуального содержания видео. В этой области пока осталось много неразрешенных вопросов. Например, отдельной темой для исследования может быть эмоциональная семантика видео Tamizharasan and S (2013). Эмоциональная семантика описывает человеческие психологические ощущения, такие как радость, гнев, страх, печаль, и пр.

Эмоциональный видео-поиск — поиск материалов, которые вызывают конкретные чувства у зрителя. Для имитации человеческого восприятия могут быть использованы новые подходы к видео-поиску.

Темой для дальнейшего изучения является мультимедийный человеко-машинный интерфейс, в частности:

-   расположение мультимедийной информации;

-   удобство интерфейса для решения задач пользователя;

-   пригодность интерфейса для оценки и обратной связи пользователей;

-   и способность интерфейса адаптироваться к привычкам запроса пользователей и отражать их индивидуальность.

Организация и визуализация результатов поиска — также интересная тема исследования. На данный момент проблема сочетания множественных информационных моделей на различных уровнях абстракции остается неразрешенной.

Эффективное использование информации о движении имеет большое значение для поиска видео. Важными задачами направления являются:

-   способность различать движения фона и переднего плана;

-   обнаружение движущиеся объектов и определять события в кадре;

-   объединение статических характеристик и характеристик движения;

-   построение индекса движения.

Интересными вопросами для исследования остаются:

-   быстрый видео-поиск с помощью иерархических индексов;

-   адаптивное обновление иерархической индексной модели;

-   обработка временных характеристик видео во время создания и обновления индекса;

-   динамические меры сходства видео на основе выбора статистических функций.


Источники
==========

Adcock, John, Andreas Girgensohn, Matthew Cooper, Ting Liu, Lynn Wilcox, and Eleanor Rieffel. 2004. “Fxpal Experiments for Trecvid 2004.” *Proceedings of the TREC Video Retrieval Evaluation (TRECVID)*: 70–81.

Andre, B., T. Vercauteren, AM. Buchner, M.B. Wallace, and N. Ayache. 2012. “Learning Semantic and Visual Similarity for Endomicroscopy Video Retrieval.” *Medical Imaging, IEEE Transactions* 31 (6) (June): 1276–1288. doi:[10.1109/TMI.2012.2188301](http://dx.doi.org/10.1109/TMI.2012.2188301).

Anjulan, A, and CN Canagarajah. 2009. “A Unified Framework for Object Retrieval and Mining.” *IEEE Transactions on Circuits and Systems for Video Technology* 19 (1): 63–76. doi:[10.1109/TCSVT.2008.2005801](http://dx.doi.org/10.1109/TCSVT.2008.2005801).

Asghar, Muhammad Nabeel, Fiaz Hussain, and Rob Manton. 2014. “Video Indexing: A Survey.” *International Journal of Computer and Information Technology* 03 (01): 148–169.

Aytar, Yusuf, Mubarak Shah, and Jiebo Luo. 2008. “Utilizing Semantic Word Similarity Measures for Video Retrieval.” *2013 IEEE Conference on Computer Vision and Pattern Recognition* 0: 1–8. doi:[http://doi.ieeecomputersociety.org/10.1109/CVPR.2008.4587822](http://dx.doi.org/http://doi.ieeecomputersociety.org/10.1109/CVPR.2008.4587822).

Browne, Paul, and Alan F. Smeaton. 2005. “Video Retrieval Using Dialogue, Keyframe Similarity and Video Objects.” In *ICIP (3)*, 1208–1211. IEEE. <http://dblp.uni-trier.de/db/conf/icip/icip2005-3.html>.

Chang, Peng, Mei Han, and Yihong Gong. 2002. “Extract Highlights From Baseball Game Video With Hidden Markov Models.” In 609–612.

Chen, Liang-Hua, Kuo-Hao Chin, and Hong-Yuan Liao. 2008. “An Integrated Approach to Video Retrieval.” In *Nineteenth Australasian Database Conference (ADC 2008)*, edited by Alan Fekete and Xuemin Lin, 75:49–55. CRPIT. Wollongong, NSW, Australia: ACS.

Chen, Liang-Hua, Yu-Chun Lai, and Hong-Yuan Mark Liao. 2008. “Movie Scene Segmentation Using Background Information.” *Pattern Recogn.* 41 (3) (March): 1056–1065. doi:[10.1016/j.patcog.2007.07.024](http://dx.doi.org/10.1016/j.patcog.2007.07.024). <http://dx.doi.org/10.1016/j.patcog.2007.07.024>.

Chen, Xu, AO. Hero, and S. Savarese. 2012. “Multimodal Video Indexing and Retrieval Using Directed Information.” *Multimedia, IEEE Transactions on* 14 (1) (Feb): 3–16. doi:[10.1109/TMM.2011.2167223](http://dx.doi.org/10.1109/TMM.2011.2167223).

Cheng, Xiangang, and Liang-Tien Chia. 2011. “Stratification-Based Keyframe Cliques for Effective and Efficient Video Representation.” *IEEE Transactions on Multimedia* 13 (6): 1333–1342.

Chu, Wei-Ta, and Shang-Yin Tsai. 2012. “Rhythm of Motion Extraction and Rhythm-Based Cross-Media Alignment for Dance Videos.” *Multimedia, IEEE Transactions on* 14 (1) (Feb): 129–141. doi:[10.1109/TMM.2011.2172401](http://dx.doi.org/10.1109/TMM.2011.2172401).

Chung, Yuk Ying, Wai Kwok Jess Chin, Xiaoming Chen, David Yu Shi, Eric Choi, and Fang Chen. 2007. “Performance Analysis of Using Wavelet Transform in Content Based Video Retrieval System.” In *Proceedings of the 2007 Annual Conference on International Conference on Computer Engineering and Applications*, 277–282. CEA’07. Stevens Point, Wisconsin, USA: World Scientific; Engineering Academy; Society (WSEAS). <http://dl.acm.org/citation.cfm?id=1348258.1348307>.

Dimitrova, Nevenka, Hong-Jiang Zhang, Behzad Shahraray, Ibrahim Sezan, Thomas Huang, and Avideh Zakhor. 2002. “Applications of Video-Content Analysis and Retrieval.” *IEEE MultiMedia* 9 (3) (July): 42–55. doi:[10.1109/MMUL.2002.1022858](http://dx.doi.org/10.1109/MMUL.2002.1022858). <http://dx.doi.org/10.1109/MMUL.2002.1022858>.

Fu, Yanwei, Yanwen Guo, Yanshu Zhu, Feng Liu, Chuanming Song, and Zhi-Hua Zhou. 2010. “Multi-View Video Summarization.” *Multimedia, IEEE Transactions on* 12 (7) (Nov): 717–729. doi:[10.1109/TMM.2010.2052025](http://dx.doi.org/10.1109/TMM.2010.2052025).

Haase, Kenneth B., Marc Eliot Davis, and Marc Eliot Davis. 1995. “Media Streams: Representing Video for Retrieval and Repurposing.”

Hauptmann, A. G., R. V. Baron, M. Y. Chen, M. Christel, P. Duygulu, C. Huang, R. Jin, et al. 2003. “Informedia at TRECVID 2003: Analyzing and Searching Broadcast News Video.” In *Proceedings of the TRECVID Workshop*. <http://www.science.uva.nl/research/publications/2003/HauptmannPTRECVID2003>.

Hong, G.Y., B. Fong, and A.C.M. Fong. 2005. “An Intelligent Video Categorization Engine.” *Kybernetes* 34 (6): 784–802. doi:[10.1108/03684920510595490](http://dx.doi.org/10.1108/03684920510595490). <http://www.emeraldinsight.com/doi/abs/10.1108/03684920510595490>.

Huurnink, B., C. G M Snoek, M. de Rijke, and A W M Smeulders. 2012. “Content-Based Analysis Improves Audiovisual Archive Retrieval.” *Multimedia, IEEE Transactions on* 14 (4) (Aug): 1166–1178. doi:[10.1109/TMM.2012.2193561](http://dx.doi.org/10.1109/TMM.2012.2193561).

Ionescu, Bogdan, Klaus Seyerlehner, Christoph Rasche, Constantin Vertan, and Patrick Lambert. 2012. “Video Genre Categorization and Representation Using Audio-Visual Information.” *SPIE - Journal of Electronic Imaging* 21(2).

Jiang, Yu-Gang, Qi Dai, Jun Wang, Chong-Wah Ngo, Xiangyang Xue, and Shih-Fu Chang. 2012. “Fast Semantic Diffusion for Large-Scale Context-Based Image and Video Annotation.” *Image Processing, IEEE Transactions on* 21 (6) (June): 3080–3091. doi:[10.1109/TIP.2012.2188038](http://dx.doi.org/10.1109/TIP.2012.2188038).

Jiang, Yu-Gang, Chong-Wah Ngo, and Jun Yang. 2007. “Towards Optimal Bag-of-Features for Object Categorization and Semantic Video Retrieval.” In *Proceedings of the 6th ACM International Conference on Image and Video Retrieval*, 494–501. CIVR ’07. New York, NY, USA: ACM. doi:[10.1145/1282280.1282352](http://dx.doi.org/10.1145/1282280.1282352). <http://doi.acm.org/10.1145/1282280.1282352>.

Kamabathula, Vijaya Kumar, and Sridhar Iyer. 2011. “Automated Tagging to Enable Fine-Grained Browsing of Lecture Videos.” *2012 IEEE Fourth International Conference on Technology for Education* 0: 96–102. doi:[http://doi.ieeecomputersociety.org/10.1109/T4E.2011.23](http://dx.doi.org/http://doi.ieeecomputersociety.org/10.1109/T4E.2011.23).

Karpenko, A, and P. Aarabi. 2011. “Tiny Videos: A Large Data Set for Nonparametric Video Retrieval and Frame Classification.” *Pattern Analysis and Machine Intelligence, IEEE Transactions on* 33 (3) (March): 618–630. doi:[10.1109/TPAMI.2010.118](http://dx.doi.org/10.1109/TPAMI.2010.118).

Li, Huiping, and D. Doermann. 2002. “Video Indexing and Retrieval Based on Recognized Text.” In *Multimedia Signal Processing, 2002 IEEE Workshop on*, 245–248. doi:[10.1109/MMSP.2002.1203292](http://dx.doi.org/10.1109/MMSP.2002.1203292).

Lim, JaeDeok, ByeongCheol Choi, SeungWan Han, and ChoelHoon Lee. 2012. “Adult Movie Classification System Based on Multimodal Approach with Visual and Auditory Features.” In *Information Science and Digital Content Technology (ICIDT), 2012 8th International Conference on*, 3:745–748.

Matsumoto, Kazunori, Masaki Naito, Keiichiro Hoashi, and Fumiaki Sugaya. 2006. “SVM-Based Shot Boundary Detection with a Novel Feature.” *2012 IEEE International Conference on Multimedia and Expo* 0: 1837–1840. doi:[http://doi.ieeecomputersociety.org/10.1109/ICME.2006.262911](http://dx.doi.org/http://doi.ieeecomputersociety.org/10.1109/ICME.2006.262911).

Nigay, Laurence, and Joëlle Coutaz. 1993. “A Design Space for Multimodal Systems: Concurrent Processing and Data Fusion.” In *Proceedings of the INTERACT ’93 and CHI ’93 Conference on Human Factors in Computing Systems*, 172–178. CHI ’93. New York, NY, USA: ACM. doi:[10.1145/169059.169143](http://dx.doi.org/10.1145/169059.169143). <http://doi.acm.org/10.1145/169059.169143>.

Quack, T., V. Ferrari, and L. Gool. 2006. “Video Mining with Frequent Item Set Configurations.” In *Int. Conf. Image Video Retrieval*, 360–369.

Repp, Stephan, Andreas Grob, and Christoph Meinel. 2008. “Browsing Within Lecture Videos Based on the Chain Index of Speech Transcription.” *IEEE Transactions on Learning Technologies* 1 (3): 145–156. doi:[http://doi.ieeecomputersociety.org/10.1109/TLT.2008.22](http://dx.doi.org/http://doi.ieeecomputersociety.org/10.1109/TLT.2008.22).

Sargin, Mehmet Emre, and Hrishikesh Aradhye. 2011. “Boosting Video Classification Using Cross-Video Signals.” In *Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference*, 1805–1808. doi:[10.1109/ICASSP.2011.5946854](http://dx.doi.org/10.1109/ICASSP.2011.5946854).

Seyerlehner, Klaus, Markus Schedl, Tim Pohle, and Peter Knees. 2010. “Using Block-Level Features For Genre Classification, Tag Classification And Music Similarity Estimation.”

Sivic, J., M. Everingham, and A. Zisserman. 2005. “Person Spotting: Video Shot Retrieval for Face Sets.” In *ACM International Conference on Image and Video Retrieval*.

Sivic, Josef, and Andrew Zisserman. 2006. “Video Google: Efficient Visual Search of Videos.” In *Toward Category-Level Object Recognition*, 127–144.

Smeaton, A. F. 2006. “Techniques Used and Open Challenges to the Analysis, Indexing and Retrieval of Digital Video.” *Information Systems* 32 (4): 545–559.

Snoek, Cees G. M., Bouke Huurnink, Laura Hollink, Maarten de Rijke, Guus Schreiber, and Marcel Worring. 2007. “Adding Semantics to Detectors for Video Retrieval.” *IEEE Transactions on Multimedia* 9 (5) (August): 975–986.

Su, Chih-Wen, H.-Y.M. Liao, Hsiao-Rong Tyan, Chia-Wen Lin, Duan-Yu Chen, and Kuo-Chin Fan. 2007. “Motion Flow-Based Video Retrieval.” *Multimedia, IEEE Transactions on* 9 (6) (Oct): 1193–1201. doi:[10.1109/TMM.2007.902875](http://dx.doi.org/10.1109/TMM.2007.902875).

Sundaram, H., and Shih-Fu Chang. 2000. “Video Scene Segmentation Using Video and Audio Features.” In *Multimedia and Expo, 2000. ICME 2000. 2000 IEEE International Conference on*, 2:1145–1148 vol.2. doi:[10.1109/ICME.2000.871563](http://dx.doi.org/10.1109/ICME.2000.871563).

Tahayna., Bashar, Mohammed Belkhatir.,M. Saadat Alhashmi, and O’DanielThomas. 2010. “Optimizing Support Vector Machine Based Classification and Retrieval of Semantic Video Events with Genetic Algorithms.” In *Image Processing (ICIP), 2010 17th IEEE International Conference on*, 1485–1488. doi:[10.1109/ICIP.2010.5653724](http://dx.doi.org/10.1109/ICIP.2010.5653724).

Tamizharasan, C, and Chandrakala S. 2013. “A Survey On Multimodal Content Based Video Retrieval.” In *International Journal of Emerging Technology and Advanced Engineering*. Vol. 3. Chennai, INDIA: Sri Sai Ram Engineering College. An ISO 9001:2008 Certified; NBA Accredited Engineering Institute.

Tian, Xinmie, Linjun Yang, Jingdong Wang, Xiuqing Wu, and Xian-Sheng Hua. 2011. “Bayesian Visual Reranking.” *Trans. Multi.* 13 (4) (August): 639–652. doi:[10.1109/TMM.2011.2111363](http://dx.doi.org/10.1109/TMM.2011.2111363). <http://dx.doi.org/10.1109/TMM.2011.2111363>.

Truong, Ba Tu, and Svetha Venkatesh. 2007. “Video Abstraction: A Systematic Review and Classification.” *ACM Trans. Multimedia Comput. Commun. Appl.* 3 (1) (February). doi:[10.1145/1198302.1198305](http://dx.doi.org/10.1145/1198302.1198305). <http://doi.acm.org/10.1145/1198302.1198305>.

Truong, Ba Tu, S. Venkatesh, and C. Dorai. 2003. “Scene Extraction in Motion Pictures.” *IEEE Trans. Cir. and Sys. for Video Technol.* 13 (1) (January): 5–15. doi:[10.1109/TCSVT.2002.808084](http://dx.doi.org/10.1109/TCSVT.2002.808084). <http://dx.doi.org/10.1109/TCSVT.2002.808084>.

Wang, Meng, R. Hong, Guangda Li, Zheng-Jun Zha, Shuicheng Yan, and Tat-Seng Chua. 2012. “Event Driven Web Video Summarization by Tag Localization and Key-Shot Identification.” *Multimedia, IEEE Transactions on* 14 (4) (Aug): 975–985. doi:[10.1109/TMM.2012.2185041](http://dx.doi.org/10.1109/TMM.2012.2185041).

Weal, M. J., D. T. Michaelides, K. Page, David C. De Roure, E. Monger, and M. Gobbi. 2012. “Semantic Annotation of Ubiquitous Learning Environments.” *IEEE Transactions on Learning Technologies* 5 (2): 143–156. doi:[http://doi.ieeecomputersociety.org/10.1109/TLT.2011.37](http://dx.doi.org/http://doi.ieeecomputersociety.org/10.1109/TLT.2011.37).

Wu, Jun, and Marcel Worring. 2012. “Efficient Genre-Specific Semantic Video Indexing.” *IEEE Transactions on Multimedia* 14 (2) (April): 291–302. doi:[10.1109/TMM.2011.2174969](http://dx.doi.org/10.1109/TMM.2011.2174969).

Yan, Rong, and Alexander G. Hauptmann. 2007. “A Review of Text and Image Retrieval Approaches for Broadcast News Video.” *Inf. Retr.* 10 (4-5): 445–484.

Yang, Linjun, Jiemin Liu, Xiaokang Yang, and Xian-Sheng Hua. 2007. “Multi-Modality Web Video Categorization.” In *Proceedings of the International Workshop on Workshop on Multimedia Information Retrieval*, 265–274. MIR ’07. New York, NY, USA: ACM. doi:[10.1145/1290082.1290119](http://dx.doi.org/10.1145/1290082.1290119). <http://doi.acm.org/10.1145/1290082.1290119>.

Yu, Hong Qing, C. Pedrinaci, S. Dietze, and J. Domingue. 2012. “Using Linked Data to Annotate and Search Educational Video Resources for Supporting Distance Learning.” *Learning Technologies, IEEE Transactions on* 5 (2) (April): 130–142. doi:[10.1109/TLT.2012.1](http://dx.doi.org/10.1109/TLT.2012.1).

Zha, Zheng-Jun, Meng Wang, Yan-Tao Zheng, Yi Yang, Richang Hong, and T.-S. Chua. 2012. “Interactive Video Indexing With Statistical Active Learning.” *Multimedia, IEEE Transactions on* 14 (1) (Feb): 17–27. doi:[10.1109/TMM.2011.2174782](http://dx.doi.org/10.1109/TMM.2011.2174782).

Zhang, Tianzhu, Changsheng Xu, Guangyu Zhu, Si Liu, and Hanqing Lu. 2012. “A Generic Framework for Video Annotation via Semi-Supervised Learning.” *IEEE Transactions on Multimedia*: 1206–1219.

Zhang, Xu-Dong, Tie-Yan Liu, Kwok-Tung Lo, and Jian Feng. 2003. “Dynamic Selection and Effective Compression of Key Frames for Video Abstraction.” *Pattern Recogn. Lett.* 24 (9-10) (June): 1523–1532. doi:[10.1016/S0167-8655(02)00391-4](http://dx.doi.org/10.1016/S0167-8655(02)00391-4). <http://dx.doi.org/10.1016/S0167-8655(02)00391-4>.
